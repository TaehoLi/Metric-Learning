{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:VERSION 0.9.76\n"
     ]
    }
   ],
   "source": [
    "# The testing module requires faiss\n",
    "# So if you don't have that, then this import will break\n",
    "from pytorch_metric_learning import losses, miners, samplers, trainers, testers\n",
    "import pytorch_metric_learning.utils.logging_presets as logging_presets\n",
    "import numpy as np\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch\n",
    "import logging\n",
    "from utils_for_examples import MLP, Identity\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "import pytorch_metric_learning\n",
    "logging.info(\"VERSION %s\"%pytorch_metric_learning.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "########## Training ##########\n",
    "##############################\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set trunk model and replace the softmax layer with an identity function\n",
    "trunk = models.resnet18(pretrained=True)\n",
    "trunk_output_size = trunk.fc.in_features\n",
    "trunk.fc = Identity()\n",
    "trunk = torch.nn.DataParallel(trunk.to(device))\n",
    "\n",
    "# Set embedder model. This takes in the output of the trunk and outputs 64 dimensional embeddings\n",
    "embedder = torch.nn.DataParallel(MLP([trunk_output_size, 64]).to(device))\n",
    "\n",
    "# Set the classifier. The classifier will take the embeddings and output a 100 dimensional vector.\n",
    "# (There are 100 classes in CIFAR100, which is the dataset we'll use in this example.)\n",
    "# We'll specify the classification loss further down in the code.\n",
    "classifier = torch.nn.DataParallel(MLP([64, 100])).to(device)\n",
    "\n",
    "# Set optimizers\n",
    "trunk_optimizer = torch.optim.Adam(trunk.parameters(), lr=0.00001, weight_decay=0.00005)\n",
    "embedder_optimizer = torch.optim.Adam(embedder.parameters(), lr=0.00001, weight_decay=0.00005)\n",
    "classifier_optimizer = torch.optim.Adam(classifier.parameters(), lr=0.00001, weight_decay=0.00005)\n",
    "\n",
    "# Set the image transforms\n",
    "train_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                    transforms.RandomResizedCrop(scale=(0.16, 1), ratio=(0.75, 1.33), size=227),\n",
    "                                    transforms.RandomHorizontalFlip(0.5),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "val_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                    transforms.CenterCrop(227),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# Set the datasets\n",
    "train_dataset = datasets.CIFAR100(root=\"CIFAR100_Dataset\", train=True, transform=train_transform, download=True)\n",
    "val_dataset = datasets.CIFAR100(root=\"CIFAR100_Dataset\", train=False, transform=val_transform, download=True)\n",
    "\n",
    "# Set the loss function\n",
    "loss = losses.TripletMarginLoss(margin=0.01)\n",
    "\n",
    "# Set the classification loss:\n",
    "classification_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Set the mining function\n",
    "miner = miners.MultiSimilarityMiner(epsilon=0.1)\n",
    "\n",
    "# Set the dataloader sampler\n",
    "sampler = samplers.MPerClassSampler(train_dataset.targets, m=4)\n",
    "\n",
    "# Set other training parameters\n",
    "batch_size = 32\n",
    "num_epochs = 2\n",
    "iterations_per_epoch = 100\n",
    "\n",
    "# Package the above stuff into dictionaries.\n",
    "models = {\"trunk\": trunk, \"embedder\": embedder, \"classifier\": classifier}\n",
    "optimizers = {\"trunk_optimizer\": trunk_optimizer, \"embedder_optimizer\": embedder_optimizer, \"classifier_optimizer\": classifier_optimizer}\n",
    "loss_funcs = {\"metric_loss\": loss, \"classifier_loss\": classification_loss}\n",
    "mining_funcs = {\"post_gradient_miner\": miner}\n",
    "\n",
    "# We can specify loss weights if we want to. This is optional\n",
    "loss_weights = {\"metric_loss\": 1, \"classifier_loss\": 0.5}\n",
    "\n",
    "record_keeper, _, _ = logging_presets.get_record_keeper(\"example_logs\", \"example_tensorboard\")\n",
    "hooks = logging_presets.get_hook_container(record_keeper)\n",
    "dataset_dict = {\"val\": val_dataset}\n",
    "model_folder = \"example_saved_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initializing dataloader\n",
      "INFO:root:Initializing dataloader iterator\n",
      "INFO:root:Done creating dataloader iterator\n",
      "INFO:root:TRAINING EPOCH 1\n",
      "total_loss=2.44327: 100%|██████████| 100/100 [00:09<00:00, 11.08it/s]\n",
      "INFO:root:Evaluating epoch 1\n",
      "INFO:root:Getting embeddings for the val split\n",
      "100%|██████████| 313/313 [00:08<00:00, 36.24it/s]\n",
      "INFO:root:Computing accuracy for the val split\n",
      "INFO:root:running k-nn with k=100\n",
      "INFO:root:embedding dimensionality is 64\n",
      "INFO:root:New best accuracy!\n",
      "INFO:root:TRAINING EPOCH 2\n",
      "total_loss=2.33122: 100%|██████████| 100/100 [00:07<00:00, 14.07it/s]\n",
      "INFO:root:Evaluating epoch 2\n",
      "INFO:root:Getting embeddings for the val split\n",
      "100%|██████████| 313/313 [00:08<00:00, 35.95it/s]\n",
      "INFO:root:Computing accuracy for the val split\n",
      "INFO:root:running k-nn with k=100\n",
      "INFO:root:embedding dimensionality is 64\n",
      "INFO:root:New best accuracy!\n"
     ]
    }
   ],
   "source": [
    "# Create the tester\n",
    "tester = testers.GlobalEmbeddingSpaceTester(end_of_testing_hook=hooks.end_of_testing_hook)\n",
    "end_of_epoch_hook = hooks.end_of_epoch_hook(tester, dataset_dict, model_folder)\n",
    "trainer = trainers.TrainWithClassifier(models,\n",
    "                                optimizers,\n",
    "                                batch_size,\n",
    "                                loss_funcs,\n",
    "                                mining_funcs,\n",
    "                                iterations_per_epoch,\n",
    "                                train_dataset,\n",
    "                                loss_weights=loss_weights,\n",
    "                                sampler=sampler,\n",
    "                                end_of_iteration_hook=hooks.end_of_iteration_hook,\n",
    "                                end_of_epoch_hook=end_of_epoch_hook)\n",
    "\n",
    "trainer.train(num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
